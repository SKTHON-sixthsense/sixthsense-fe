import { google } from "@ai-sdk/google";
import { generateObject } from "ai";
import { NextRequest, NextResponse } from "next/server";
import { z } from "zod";
import OpenAI from "openai";

// Available routes and their descriptions for the AI assistant
const AVAILABLE_ROUTES = {
  "/": "í™ˆí˜ì´ì§€ - ë©”ì¸ í˜ì´ì§€, êµ¬ì¸ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤",
  "/home": "í™ˆ - ê¸°ë³¸ í™ˆ í˜ì´ì§€",
  "/my": "ë‚˜ì˜ ì •ë³´ - ì‚¬ìš©ìì˜ ê¸°ë³¸ ì •ë³´ì™€ ìê¸°ì†Œê°œì„œë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
  "/my/edit/general": "ê¸°ë³¸ ì •ë³´ ìˆ˜ì • - ê°œì¸ ì •ë³´ë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
  "/my/edit/general/health": "ê±´ê°• ìƒíƒœ ì„ íƒ - ê±´ê°• ìƒíƒœë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
  "/my/edit/general/personality": "ì„±ê²© ì„ íƒ - ì„±ê²© ìœ í˜•ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
  "/onboarding": "ì˜¨ë³´ë”© - ìƒˆ ì‚¬ìš©ìë¥¼ ìœ„í•œ ì´ˆê¸° ì„¤ì • ê³¼ì •ì…ë‹ˆë‹¤",
};

// Schema for structured output with routing queue support
const NavigationResponseSchema = z.object({
  transcription: z.string().describe("ì „ì‚¬ëœ í…ìŠ¤íŠ¸"),
  intent: z.string().describe("ì‚¬ìš©ìì˜ ì˜ë„ ë¶„ì„"),
  routingQueue: z
    .array(
      z.object({
        route: z
          .string()
          .describe("ì´ë™í•  ê²½ë¡œ (ì •í™•í•œ ê²½ë¡œë§Œ, ì˜ˆ: /my) ë˜ëŠ” íŠ¹ìˆ˜ ëª…ë ¹ (BACK, FORWARD)"),
        delay: z.number().describe("ì´ì „ í˜ì´ì§€ì—ì„œ ë¨¸ë¬´ë¥¼ ì‹œê°„ (ë°€ë¦¬ì´ˆ, ê¸°ë³¸ê°’ 1500)"),
      })
    )
    .describe("ìˆœì„œëŒ€ë¡œ ì´ë™í•  ê²½ë¡œë“¤ì˜ ë°°ì—´"),
});

export async function POST(req: NextRequest) {
  try {
    const formData = await req.formData();
    const audioFile = formData.get("audio") as File;

    console.log("ğŸ¤ AI Assist Request:", {
      audioFileSize: audioFile?.size,
      audioFileType: audioFile?.type,
      timestamp: new Date().toISOString(),
    });

    if (!audioFile) {
      console.log("âŒ No audio file provided");
      return NextResponse.json({ error: "No audio file provided" }, { status: 400 });
    }

    // Initialize OpenAI client for STT
    const openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });

    // First, transcribe the audio using OpenAI Whisper
    console.log("ğŸ”Š Starting transcription with OpenAI Whisper...");
    const transcriptionResult = await openai.audio.transcriptions.create({
      file: audioFile,
      model: "gpt-4o-mini-transcribe",
      language: "ko", // Korean language
      response_format: "text",
    });

    const transcribedText = transcriptionResult.trim();

    console.log("ğŸ“ Transcription:", transcribedText);

    if (!transcribedText) {
      console.log("âŒ Could not transcribe audio");
      return NextResponse.json({ error: "Could not transcribe audio" }, { status: 400 });
    }

    // Now analyze the transcribed text to determine navigation intent
    const routesList = Object.entries(AVAILABLE_ROUTES)
      .map(([route, description]) => `- ${route}: ${description}`)
      .join("\n");

    const navigationResult = await generateObject({
      model: google("gemini-2.5-flash"),
      schema: NavigationResponseSchema,
      prompt: `
ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ë§í•œ ë‚´ìš©ì…ë‹ˆë‹¤: "${transcribedText}"

ì´ ì›¹ì‚¬ì´íŠ¸ëŠ” 5060 ì„¸ëŒ€ë¥¼ ìœ„í•œ ì»¤ë¦¬ì–´ í”Œë«í¼ "ë‹¤ì‹œì¡"ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ í˜ì´ì§€ë¡œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ í˜ì´ì§€ë“¤:
${routesList}

## í˜ì´ì§€ ë§¤í•‘ ê·œì¹™:
- "ë‚˜ì˜ ì •ë³´", "ë‚´ ì •ë³´", "ë§ˆì´í˜ì´ì§€", "í”„ë¡œí•„" -> /my
- "í™ˆ", "ë©”ì¸", "ì²˜ìŒ", "í™ˆí˜ì´ì§€" -> /
- "ì˜¨ë³´ë”©", "ì‹œì‘", "ì„¤ì •" -> /onboarding  
- "êµ¬ì¸", "ì¼ìë¦¬", "ì±„ìš©" -> /
- "ê¸°ë³¸ ì •ë³´ ìˆ˜ì •", "ê°œì¸ì •ë³´ ìˆ˜ì •" -> /my/edit/general
- "ê±´ê°• ìƒíƒœ", "ê±´ê°• ì„¤ì •" -> /my/edit/general/health
- "ì„±ê²© ì„ íƒ", "ì„±ê²© ì„¤ì •" -> /my/edit/general/personality

## íŠ¹ìˆ˜ ë„¤ë¹„ê²Œì´ì…˜ ëª…ë ¹:
- "ë’¤ë¡œ", "ë’¤ë¡œ ê°€ê¸°", "ì´ì „", "ì´ì „ í˜ì´ì§€" -> BACK
- "ì•ìœ¼ë¡œ", "ì•ìœ¼ë¡œ ê°€ê¸°", "ë‹¤ìŒ", "ë‹¤ì‹œ ì•ìœ¼ë¡œ" -> FORWARD

## ë‹¤ì¤‘ ê²½ë¡œ ì²˜ë¦¬:
ì‚¬ìš©ìê°€ ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ìˆœì„œëŒ€ë¡œ ë°©ë¬¸í•˜ê³  ì‹¶ì–´í•˜ëŠ” ê²½ìš° (ì˜ˆ: "í™ˆìœ¼ë¡œ ê°”ë‹¤ê°€ ë§ˆì´í˜ì´ì§€ë¡œ ê°€ì¤˜", "ë¨¼ì € ì˜¨ë³´ë”© ë³´ê³  ë‚˜ì„œ ë‚´ ì •ë³´ ìˆ˜ì •í•´ì¤˜"), routingQueue ë°°ì—´ì— ìˆœì„œëŒ€ë¡œ ì¶”ê°€í•˜ì„¸ìš”.

ê° ê²½ë¡œ ê°ì²´ëŠ”:
- route: ì •í™•í•œ ê²½ë¡œ (ì˜ˆ: "/", "/my") ë˜ëŠ” íŠ¹ìˆ˜ ëª…ë ¹ ("BACK", "FORWARD"),
- delay: ì´ì „ í˜ì´ì§€ì—ì„œ ë¨¸ë¬´ë¥¼ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
  - ì²« ë²ˆì§¸ ê²½ë¡œ: 0 (ì¦‰ì‹œ ì´ë™)
  - ì´í›„ ê²½ë¡œë“¤: 1500-3000ms (í˜ì´ì§€ í™•ì¸ ì‹œê°„)

## ì‘ë‹µ í˜•ì‹:
transcription: "${transcribedText}" (ê·¸ëŒ€ë¡œ ì…ë ¥)
intent: ì‚¬ìš©ì ì˜ë„ ë¶„ì„
routingQueue: [{ route: "ê²½ë¡œ", delay: ì‹œê°„ }, ...]

## ì˜ˆì‹œ:
"í™ˆìœ¼ë¡œ ê°”ë‹¤ê°€ ë§ˆì´í˜ì´ì§€ë¡œ ê°€ì¤˜" â†’ 
routingQueue: [
  { route: "/", delay: 0 },
  { route: "/my", delay: 2000 }
]

"ë’¤ë¡œ ê°€ê¸°" â†’
routingQueue: [
  { route: "BACK", delay: 0 }
]

"ë’¤ë¡œ ê°”ë‹¤ê°€ ë‹¤ì‹œ ì•ìœ¼ë¡œ ê°€ì¤˜" â†’
routingQueue: [
  { route: "BACK", delay: 0 },
  { route: "FORWARD", delay: 1500 }
]
      `,
    });

    console.log("ğŸ¯ Navigation Result:", {
      transcription: navigationResult.object.transcription,
      intent: navigationResult.object.intent,
      routingQueue: navigationResult.object.routingQueue,
    });

    return NextResponse.json(navigationResult.object);
  } catch (error) {
    console.error("âŒ AI assist error:", error);
    return NextResponse.json({ error: "Internal server error" }, { status: 500 });
  }
}
